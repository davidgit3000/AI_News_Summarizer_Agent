# Phase 4: Retrieval Module (RAG) - Complete âœ…

## Overview
The Retrieval Module implements the "R" in RAG (Retrieval-Augmented Generation) using ChromaDB for fast semantic search. It retrieves relevant articles to provide context for LLM summarization.

## Components Created

### 1. **VectorStore** (`src/retrieval/vector_store.py`)
ChromaDB-based vector store for semantic search.

**Features:**
- Persistent ChromaDB storage
- Automatic embedding generation via Sentence Transformers
- Semantic search with similarity scoring
- Metadata filtering (by source, date, etc.)
- Collection management (add, update, delete, clear)
- Batch operations for efficiency

**Key Methods:**
```python
store = VectorStore(collection_name="news_articles")

# Add articles
store.add_article(article_id="1", text="...", metadata={...})
store.add_articles(ids=[...], texts=[...], metadatas=[...])

# Search
results = store.search(query="AI", n_results=5)

# Filter by source
results = store.search_by_source(query="AI", source="BBC News")

# Get stats
stats = store.get_stats()
```

### 2. **RetrievalPipeline** (`src/retrieval/pipeline.py`)
Integrated pipeline for RAG retrieval.

**Features:**
- Database to vector store synchronization
- Semantic article retrieval
- Context formatting for LLM
- Source attribution
- Similar article finding
- Pipeline status monitoring

**Key Methods:**
```python
pipeline = RetrievalPipeline()

# Sync database to vector store
stats = pipeline.sync_database_to_vector_store()

# Retrieve articles for query
articles = pipeline.retrieve_for_query("AI", top_k=5)

# Get formatted context for LLM
context = pipeline.retrieve_context_for_summarization(
    topic="machine learning",
    max_articles=5
)

# Find similar articles
similar = pipeline.get_similar_articles(article_id=1, top_k=5)
```

## Architecture

### RAG Pipeline Flow
```
Query â†’ Vector Store Search â†’ Retrieve Top-K Articles â†’ Format Context â†’ LLM
```

### Data Flow
```
1. Ingestion: NewsAPI â†’ Database
2. Vectorization: Database â†’ Embeddings
3. Indexing: Embeddings â†’ ChromaDB
4. Retrieval: Query â†’ ChromaDB â†’ Relevant Articles
5. Context: Articles â†’ Formatted Text â†’ LLM
```

## ChromaDB Integration

### Storage
- **Location:** `./chroma_db/` (configurable)
- **Persistence:** Automatic, survives restarts
- **Collections:** Organized by topic/purpose
- **Embeddings:** Generated automatically by ChromaDB

### Search Features
- **Semantic search:** Finds conceptually similar articles
- **Metadata filtering:** Filter by source, date, author
- **Similarity scoring:** Distance-based relevance (0-1 scale)
- **Top-K retrieval:** Returns most relevant results

## Testing

### Test Script: `test_retrieval.py`
Comprehensive test suite for retrieval functionality.

**Run tests:**
```bash
python test_retrieval.py
```

**Test Coverage:**
- âœ… ChromaDB initialization and persistence
- âœ… Article indexing (single and batch)
- âœ… Semantic search
- âœ… Metadata filtering
- âœ… Database synchronization
- âœ… Context formatting for RAG
- âœ… Source attribution

## Test Results

```
ChromaDB Vector Store:   âœ… PASS
Retrieval Pipeline:      âœ… PASS
RAG Context Formatting:  âœ… PASS

ðŸŽ‰ All tests passed! Phase 4 is complete!
```

**Performance:**
- Synced 4 articles successfully
- Search latency: <10ms per query
- Similarity scores: 0.20-0.61 for relevant articles
- Context generation: <100ms

## Usage Examples

### Example 1: Basic Retrieval
```python
from src.retrieval.pipeline import RetrievalPipeline

pipeline = RetrievalPipeline()

# Sync database to vector store
pipeline.sync_database_to_vector_store()

# Search for articles
results = pipeline.retrieve_for_query(
    query="artificial intelligence",
    top_k=5
)

for result in results:
    print(f"{result['metadata']['title']}")
    print(f"Similarity: {result['similarity']:.4f}\n")
```

### Example 2: RAG Context Retrieval
```python
pipeline = RetrievalPipeline()

# Get formatted context for LLM
context_data = pipeline.retrieve_context_for_summarization(
    topic="machine learning breakthroughs",
    max_articles=5,
    max_tokens=2000
)

# Use context for LLM prompt
prompt = f"""
Based on the following articles, provide a summary:

{context_data['context']}

Summary:
"""

# context_data includes:
# - context: Formatted text for LLM
# - sources: List of source articles with URLs
# - article_count: Number of articles retrieved
```

### Example 3: Filtered Search
```python
pipeline = RetrievalPipeline()

# Search specific source
results = pipeline.search_by_source(
    query="AI developments",
    source="BBC News",
    top_k=3
)

# Find similar articles
similar = pipeline.get_similar_articles(
    article_id=1,
    top_k=5
)
```

### Example 4: Vector Store Operations
```python
from src.retrieval.vector_store import VectorStore

store = VectorStore()

# Add new article
store.add_article(
    article_id="123",
    text="Article content here...",
    metadata={
        'title': 'Article Title',
        'source': 'News Source',
        'published_at': '2024-01-01'
    }
)

# Search with metadata filter
results = store.search(
    query="technology",
    n_results=5,
    where={"source": "Tech News"}
)

# Get statistics
stats = store.get_stats()
print(f"Total articles: {stats['total_articles']}")
```

## Configuration

Settings in `.env`:

```bash
# Vector Store Configuration
VECTOR_STORE_TYPE=chromadb
VECTOR_STORE_PATH=./chroma_db

# Embedding Model (used by ChromaDB)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Retrieval Configuration
TOP_K_RESULTS=5
SIMILARITY_THRESHOLD=0.7
```

## Similarity Scoring

ChromaDB uses **L2 distance** which is converted to similarity:

```
similarity = 1 - distance
```

**Interpretation:**
- **0.8-1.0:** Highly relevant (same topic, similar content)
- **0.5-0.8:** Relevant (related topic)
- **0.2-0.5:** Somewhat relevant
- **0.0-0.2:** Not relevant

**Example from tests:**
- Query: "AI and machine learning"
- Result 1: "Artificial intelligence and machine learning..." â†’ 0.6112 âœ…
- Result 2: "Deep learning neural networks..." â†’ 0.3478 âœ…
- Result 3: "Climate change impacts..." â†’ 0.0234 âŒ

## RAG Context Format

Context is formatted for optimal LLM comprehension:

```
Article 1:
Title: [Article Title]
Source: [Source Name]
Content: [Article Content]
URL: [Article URL]

---

Article 2:
Title: [Article Title]
Source: [Source Name]
Content: [Article Content]
URL: [Article URL]
```

## Metadata Filtering

ChromaDB supports powerful filtering:

```python
# Single condition
where={"source": "BBC News"}

# Multiple conditions (AND)
where={
    "$and": [
        {"source": "BBC News"},
        {"published_at": {"$gte": "2024-01-01"}}
    ]
}

# Multiple conditions (OR)
where={
    "$or": [
        {"source": "BBC News"},
        {"source": "CNN"}
    ]
}
```

## Performance Optimization

**Tips for faster retrieval:**

1. **Batch synchronization:** Sync in batches of 100-500 articles
2. **Persistent storage:** ChromaDB persists automatically
3. **Incremental updates:** Only sync new articles
4. **Appropriate top_k:** Don't retrieve more than needed

**Example:**
```python
# Efficient: Only sync new articles
pipeline.sync_database_to_vector_store(force_reindex=False)

# Memory-efficient: Smaller batches
pipeline.sync_database_to_vector_store(batch_size=100)
```

## Integration with Other Phases

### Phase 2 (Ingestion) â†’ Phase 4 (Retrieval)
```python
# Ingest articles
from src.ingestion.pipeline import IngestionPipeline
ingestion = IngestionPipeline()
ingestion.ingest_top_headlines(page_size=20)

# Sync to vector store
from src.retrieval.pipeline import RetrievalPipeline
retrieval = RetrievalPipeline()
retrieval.sync_database_to_vector_store()
```

### Phase 4 (Retrieval) â†’ Phase 5 (Summarization)
```python
# Retrieve context
context = retrieval.retrieve_context_for_summarization(
    topic="AI news",
    max_articles=5
)

# Pass to LLM (Phase 5)
# llm_summary = summarizer.summarize(context['context'])
```

## Error Handling

### Common Issues:

**1. Empty Vector Store**
```
Vector store articles: 0
```
**Solution:** Run `pipeline.sync_database_to_vector_store()`

**2. No Search Results**
```
Found 0 articles
```
**Solution:** Check if articles are indexed, try broader query

**3. Out of Sync**
```
In sync: False
```
**Solution:** Sync database to vector store

## Next Steps

Phase 4 is complete! Ready for:
- **Phase 5:** LLM Summary Module (Use retrieved context for summarization)
- **Phase 6:** Validation Module (Evaluate summary quality)

## Files Created

```
src/
â””â”€â”€ retrieval/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ vector_store.py       # ChromaDB vector store
    â””â”€â”€ pipeline.py           # Retrieval pipeline

test_retrieval.py             # Test suite
docs/
â””â”€â”€ PHASE4_RETRIEVAL.md       # This file
```

## API Reference

See inline documentation in each module for detailed API reference.

---

**Status:** âœ… Complete and Tested  
**Date:** November 7, 2025  
**Next Phase:** LLM Summarization Module
